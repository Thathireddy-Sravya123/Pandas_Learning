{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqSBn9EVW/dstBnLicqZpT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Thathireddy-Sravya123/Pandas_Learning/blob/main/Web_Scraping_using_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Web Scraping using Python :**\n",
        "\n",
        "Web scraping is essentially the process of automatically collecting data from websites.\n",
        "\n",
        "Imagine you want to gather product information from an e-commerce site, or news articles from a publication. Web scraping automates this by using a software tool to extract the specific data you need, instead of manually copying and pasting it yourself.\n",
        "\n",
        "Here's a breakdown of how it works:\n",
        "\n",
        "*   Data Extraction bold text\n",
        "*   Tools and Techniques\n",
        "\n",
        "It's important to remember that scraping should be done responsibly.  Always check the website's terms and conditions to ensure scraping is allowed, and avoid overloading the site with too many requests\n",
        "\n"
      ],
      "metadata": {
        "id": "3g-p9V4RYl2x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Modules in Web Scraping :**\n",
        "\n",
        "\n",
        "There are several key modules used for web scraping, primarily in Python:\n",
        "\n",
        "**Requests**: This library handles sending HTTP requests (like GET or POST) to websites and retrieving the response content. It's a popular choice for its simplicity and ease of use.\n",
        "\n",
        "**BeautifulSoup**: This library excels at parsing HTML and XML content. It helps navigate the structure of the downloaded webpage, allowing you to target specific elements containing the desired data."
      ],
      "metadata": {
        "id": "pWFCyfjkcCV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code for Web Scraping**\n",
        "\n",
        "This code performs the following steps:\n",
        "\n",
        "1. Imports requests and BeautifulSoup libraries.\n",
        "2. Defines the url variable for the target webpage.\n",
        "3. Sends a GET request using requests.get and stores the response in a variable.\n",
        "4. Checks the response status code. If it's 200 (success), proceed with parsing the content.\n",
        "5. Creates a BeautifulSoup object using the response content.\n",
        "6. Finds all elements containing product information using find_all. You'll need to adjust the tag and class name based on the website's HTML structure.\n",
        "7. Iterates through each product element and searches for the title element using find. Again, adjust the tag and class name as needed.\n",
        "Extracts the text content of the title element using .text and strips whitespace with .strip().\n",
        "8. Appends the extracted title to the titles list.\n",
        "9. Prints the final list of product titles or an error message if the request fails."
      ],
      "metadata": {
        "id": "-GlAIebFqJ-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Define the target URL\n",
        "url = \"https://www.example.com/products\"\n",
        "\n",
        "# Send an HTTP GET request\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check for successful response\n",
        "if response.status_code == 200:\n",
        "  # Parse the HTML content\n",
        "  soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "  # Find all product elements (replace 'div' with the appropriate tag based on website structure)\n",
        "  products = soup.find_all('div', class_=\"product-item\")  # Adjust the class name as needed\n",
        "\n",
        "  # Extract product titles\n",
        "  titles = []\n",
        "  for product in products:\n",
        "    title_element = product.find('h3', class_=\"product-title\")  # Adjust the class name as needed\n",
        "    if title_element:\n",
        "      titles.append(title_element.text.strip())\n",
        "\n",
        "  # Print the extracted titles\n",
        "  print(titles)\n",
        "else:\n",
        "  print(\"Error:\", response.status_code)\n"
      ],
      "metadata": {
        "id": "RVWNd2WRpaB5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}